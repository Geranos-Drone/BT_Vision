{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Title: Keypoint Detection with Transfer Learning\n",
    "Author: [Sayak Paul](https://twitter.com/RisingSayak)\n",
    "Date created: 2021/05/02\n",
    "Last modified: 2021/05/02\n",
    "Description: Training a keypoint detector with data augmentation and transfer learning.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Keypoint detection consists of locating key object parts. For example, the key parts\n",
    "of our faces include nose tips, eyebrows, eye corners, and so on. These parts help to\n",
    "represent the underlying object in a feature-rich manner. Keypoint detection has\n",
    "applications that include pose estimation, face detection, etc.\n",
    "In this example, we will build a keypoint detector using the\n",
    "[StanfordExtra dataset](https://github.com/benjiebob/StanfordExtra),\n",
    "using transfer learning. This example requires TensorFlow 2.4 or higher,\n",
    "as well as [`imgaug`](https://imgaug.readthedocs.io/) library,\n",
    "which can be installed using the following command:\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"shell\n",
    "pip install -q -U imgaug\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "## Data collection\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "The StanfordExtra dataset contains 12,000 images of dogs together with keypoints and\n",
    "segmentation maps. It is developed from the [Stanford dogs dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/).\n",
    "It can be downloaded with the command below:\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"shell\n",
    "wget -q http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Annotations are provided as a single JSON file in the StanfordExtra dataset and one needs\n",
    "to fill [this form](https://forms.gle/sRtbicgxsWvRtRmUA) to get access to it. The\n",
    "authors explicitly instruct users not to share the JSON file, and this example respects this wish:\n",
    "you should obtain the JSON file yourself.\n",
    "The JSON file is expected to be locally available as `stanfordextra_v12.zip`.\n",
    "After the files are downloaded, we can extract the archives.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"shell\n",
    "tar xf images.tar\n",
    "unzip -qq ~/stanfordextra_v12.zip\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "## Imports\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from imgaug.augmentables.kps import KeypointsOnImage\n",
    "from imgaug.augmentables.kps import Keypoint\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Define hyperparameters\n",
    "\"\"\"\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 200\n",
    "NUM_KEYPOINTS = 8 # 9 keypoints each having x and y coordinates"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Load data\n",
    "The authors also provide a metadata file that specifies additional information about the\n",
    "keypoints, like color information, animal pose name, etc. We will load this file in a `pandas`\n",
    "dataframe to extract information for visualization purposes.\n",
    "\"\"\"\n",
    "\n",
    "IMG_DIR = \"/home/tim/BT_Vision/convert_to_coco/test_dataset_coco/images/train\"\n",
    "IMG_DIR_VAL = \"/home/tim/BT_Vision/convert_to_coco/test_dataset_coco/images/val\"\n",
    "JSON = \"/home/tim/BT_Vision/convert_to_coco/test_dataset_coco/annotations/pole_keypoints_8_train.json\"\n",
    "JSON_VAL = \"/home/tim/BT_Vision/convert_to_coco/test_dataset_coco/annotations/pole_keypoints_8_val.json\"\n",
    "KEYPOINT_DEF = (\n",
    "    \"keypoint_definitions.csv\"\n",
    ")\n",
    "\n",
    "# Load the ground-truth annotations.\n",
    "with open(JSON) as infile:\n",
    "    coco_data = json.load(infile)\n",
    "\n",
    "with open(JSON_VAL) as infile:\n",
    "    coco_data_val = json.load(infile)\n",
    "\n",
    "# Set up a dictionary, mapping all the ground-truth information\n",
    "# with respect to the path of the image.\n",
    "\n",
    "def coco_to_json_data(coco_data, IMG_DIR):\n",
    "    coco_annotations = coco_data[\"annotations\"]\n",
    "    coco_images = coco_data[\"images\"]\n",
    "    json_data = []\n",
    "    for entry in coco_annotations:\n",
    "        img_bbox = entry[\"bbox\"]\n",
    "        joints = []\n",
    "        for i in range(NUM_KEYPOINTS):\n",
    "            joints.append([entry[\"keypoints\"][3*i],entry[\"keypoints\"][3*i+1],entry[\"keypoints\"][3*i+2]])\n",
    "        for img in coco_images:\n",
    "            if img[\"id\"] == entry[\"image_id\"]:\n",
    "                image_name = img[\"file_name\"]\n",
    "                image_path = os.path.join(IMG_DIR, image_name)\n",
    "                img_width = img[\"width\"]\n",
    "                img_height = img[\"height\"]\n",
    "        json_data_entry = {\n",
    "            \"img_bbox\": img_bbox,\n",
    "            \"img_height\": img_height,\n",
    "            \"img_width\": img_width,\n",
    "            \"img_path\": image_path,\n",
    "            \"joints\": joints,\n",
    "        }\n",
    "        json_data.append(json_data_entry)\n",
    "    json_dict = {i[\"img_path\"]: i for i in json_data}\n",
    "    return json_data, json_dict\n",
    "\n",
    "\n",
    "json_data, json_dict = coco_to_json_data(coco_data, IMG_DIR)\n",
    "json_data_val, json_dict_val = coco_to_json_data(coco_data_val, IMG_DIR_VAL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A single entry of `json_dict` looks like the following:\n",
    "```\n",
    "'n02085782-Japanese_spaniel/n02085782_2886.jpg':\n",
    "{'img_bbox': [205, 20, 116, 201],\n",
    " 'img_height': 272,\n",
    " 'img_path': 'n02085782-Japanese_spaniel/n02085782_2886.jpg',\n",
    " 'img_width': 350,\n",
    " 'is_multiple_dogs': False,\n",
    " 'joints': [[108.66666666666667, 252.0, 1],\n",
    "            [147.66666666666666, 229.0, 1],\n",
    "            [163.5, 208.5, 1],\n",
    "            [0, 0, 0],\n",
    "            [0, 0, 0],\n",
    "            [0, 0, 0],\n",
    "            [54.0, 244.0, 1],\n",
    "            [77.33333333333333, 225.33333333333334, 1],\n",
    "            [79.0, 196.5, 1],\n",
    "            [0, 0, 0],\n",
    "            [0, 0, 0],\n",
    "            [0, 0, 0],\n",
    "            [0, 0, 0],\n",
    "            [0, 0, 0],\n",
    "            [150.66666666666666, 86.66666666666667, 1],\n",
    "            [88.66666666666667, 73.0, 1],\n",
    "            [116.0, 106.33333333333333, 1],\n",
    "            [109.0, 123.33333333333333, 1],\n",
    "            [0, 0, 0],\n",
    "            [0, 0, 0],\n",
    "            [0, 0, 0],\n",
    "            [0, 0, 0],\n",
    "            [0, 0, 0],\n",
    "            [0, 0, 0]],\n",
    " 'seg': ...}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "In this example, the keys we are interested in are:\n",
    "* `img_path`\n",
    "* `joints`\n",
    "There are a total of NUM_KEYPOINTS entries present inside `joints`. Each entry has 3 values:\n",
    "* x-coordinate\n",
    "* y-coordinate\n",
    "* visibility flag of the keypoints (1 indicates visibility and 0 indicates non-visibility)\n",
    "As we can see `joints` contain multiple `[0, 0, 0]` entries which denote that those\n",
    "keypoints were not labeled. In this example, we will consider both non-visible as well as\n",
    "unlabeled keypoints in order to allow mini-batch learning."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the metdata definition file and preview it.\n",
    "keypoint_def = pd.read_csv(KEYPOINT_DEF)\n",
    "keypoint_def.head()\n",
    "\n",
    "# Extract the colours and labels.\n",
    "colours = keypoint_def[\"Hex_colour\"].values.tolist()\n",
    "colours = [\"#\" + colour for colour in colours]\n",
    "labels = keypoint_def[\"Name\"].values.tolist()\n",
    "\n",
    "# Utility for reading an image and for getting its annotations.\n",
    "\n",
    "def get_pole(name):\n",
    "    for i in json_dict.keys():\n",
    "        if i == name:\n",
    "            data = json_dict[name]\n",
    "    for i in json_dict_val.keys():\n",
    "        if i == name:\n",
    "            data = json_dict_val[name]\n",
    "    img_data = plt.imread(os.path.join(IMG_DIR, data[\"img_path\"]))\n",
    "    # If the image is RGBA convert it to RGB.\n",
    "    if img_data.shape[-1] == 4:\n",
    "        img_data = img_data.astype(np.uint8)\n",
    "        img_data = Image.fromarray(img_data)\n",
    "        img_data = np.array(img_data.convert(\"RGB\"))\n",
    "    data[\"img_data\"] = img_data\n",
    "\n",
    "    return data\n",
    "\n",
    "# Load the metdata definition file and preview it.\n",
    "keypoint_def = pd.read_csv(KEYPOINT_DEF)\n",
    "keypoint_def.head()\n",
    "\n",
    "# Extract the colours and labels.\n",
    "colours = keypoint_def[\"Hex_colour\"].values.tolist()\n",
    "colours = [\"#\" + colour for colour in colours]\n",
    "labels = keypoint_def[\"Name\"].values.tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Visualize data\n",
    "Now, we write a utility function to visualize the images and their keypoints.\n",
    "\"\"\"\n",
    "\n",
    "# Parts of this code come from here:\n",
    "# https://github.com/benjiebob/StanfordExtra/blob/master/demo.ipynb\n",
    "def visualize_keypoints(images, keypoints):\n",
    "    fig, axes = plt.subplots(nrows=len(images), ncols=2, figsize=(16, 12))\n",
    "    [ax.axis(\"off\") for ax in np.ravel(axes)]\n",
    "\n",
    "    for (ax_orig, ax_all), image, current_keypoint in zip(axes, images, keypoints):\n",
    "        ax_orig.imshow(image)\n",
    "        ax_all.imshow(image)\n",
    "\n",
    "        # If the keypoints were formed by `imgaug` then the coordinates need\n",
    "        # to be iterated differently.\n",
    "        if isinstance(current_keypoint, KeypointsOnImage):\n",
    "            for idx, kp in enumerate(current_keypoint.keypoints):\n",
    "                ax_all.scatter(\n",
    "                    [kp.x], [kp.y], c=colours[idx], marker=\"x\", s=50, linewidths=5\n",
    "                )\n",
    "        else:\n",
    "            current_keypoint = np.array(current_keypoint)\n",
    "            # Since the last entry is the visibility flag, we discard it.\n",
    "            current_keypoint = current_keypoint[:, :2]\n",
    "            for idx, (x, y) in enumerate(current_keypoint):\n",
    "                ax_all.scatter([x], [y], c=colours[idx], marker=\"x\", s=50, linewidths=5)\n",
    "\n",
    "    plt.tight_layout(pad=2.0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Select four samples randomly for visualization.\n",
    "samples = list(json_dict.keys())\n",
    "samples_val = list(json_dict_val.keys())\n",
    "num_samples = 4\n",
    "selected_samples = np.random.choice(samples, num_samples, replace=False)\n",
    "\n",
    "images, keypoints = [], []\n",
    "\n",
    "for sample in selected_samples:\n",
    "    data = get_pole(sample)\n",
    "    image = data[\"img_data\"]\n",
    "    keypoint = data[\"joints\"]\n",
    "\n",
    "    images.append(image)\n",
    "    keypoints.append(keypoint)\n",
    "\n",
    "visualize_keypoints(images, keypoints)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Prepare data generator\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class KeyPointsDataset(keras.utils.Sequence):\n",
    "    def __init__(self, image_keys, aug, batch_size=BATCH_SIZE, train=True):\n",
    "        self.image_keys = image_keys\n",
    "        self.aug = aug\n",
    "        self.batch_size = batch_size\n",
    "        self.train = train\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_keys) // self.batch_size\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.image_keys))\n",
    "        if self.train:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        image_keys_temp = [self.image_keys[k] for k in indexes]\n",
    "        (images, keypoints) = self.__data_generation(image_keys_temp)\n",
    "\n",
    "        return (images, keypoints)\n",
    "\n",
    "    def __data_generation(self, image_keys_temp):\n",
    "        batch_images = np.empty((self.batch_size, IMG_SIZE, IMG_SIZE, 3), dtype=\"int\")\n",
    "        batch_keypoints = np.empty(\n",
    "            (self.batch_size, 1, 1, NUM_KEYPOINTS*2), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        for i, key in enumerate(image_keys_temp):\n",
    "            data = get_pole(key)\n",
    "            current_keypoint = np.array(data[\"joints\"])[:, :2]\n",
    "            kps = []\n",
    "\n",
    "            # To apply our data augmentation pipeline, we first need to\n",
    "            # form Keypoint objects with the original coordinates.\n",
    "            for j in range(0, len(current_keypoint)):\n",
    "                kps.append(Keypoint(x=current_keypoint[j][0], y=current_keypoint[j][1]))\n",
    "\n",
    "            # We then project the original image and its keypoint coordinates.\n",
    "            current_image = data[\"img_data\"]\n",
    "            kps_obj = KeypointsOnImage(kps, shape=current_image.shape)\n",
    "\n",
    "            # Apply the augmentation pipeline.\n",
    "            (new_image, new_kps_obj) = self.aug(image=current_image, keypoints=kps_obj)\n",
    "            batch_images[\n",
    "                i,\n",
    "            ] = new_image\n",
    "\n",
    "            # Parse the coordinates from the new keypoint object.\n",
    "            kp_temp = []\n",
    "            for keypoint in new_kps_obj:\n",
    "                kp_temp.append(np.nan_to_num(keypoint.x))\n",
    "                kp_temp.append(np.nan_to_num(keypoint.y))\n",
    "\n",
    "            # More on why this reshaping later.\n",
    "            batch_keypoints[i,] = np.array(\n",
    "                kp_temp\n",
    "            ).reshape(1, 1, NUM_KEYPOINTS * 2)\n",
    "\n",
    "        # Scale the coordinates to [0, 1] range.\n",
    "        batch_keypoints = batch_keypoints / IMG_SIZE\n",
    "\n",
    "        return (batch_images, batch_keypoints)\n",
    "\n",
    "# doc keypoints in imgaug: [this document](https://imgaug.readthedocs.io/en/latest/source/examples_keypoints.html).\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Define augmentation transforms\n",
    "\"\"\"\n",
    "\n",
    "train_aug = iaa.Sequential(\n",
    "    [\n",
    "        iaa.Resize(IMG_SIZE, interpolation=\"linear\"),\n",
    "        iaa.Fliplr(0.3),\n",
    "        # `Sometimes()` applies a function randomly to the inputs with\n",
    "        # a given probability (0.3, in this case).\n",
    "        iaa.Sometimes(0.3, iaa.Affine(rotate=10, scale=(0.5, 0.7))),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_aug = iaa.Sequential([iaa.Resize(IMG_SIZE, interpolation=\"linear\")])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Create training and validation splits\n",
    "\"\"\"\n",
    "\n",
    "np.random.shuffle(samples)\n",
    "np.random.shuffle(samples_val)\n",
    "\n",
    "train_keys = samples\n",
    "validation_keys = samples_val"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Data generator investigation\n",
    "\"\"\"\n",
    "\n",
    "train_dataset = KeyPointsDataset(train_keys, train_aug)\n",
    "validation_dataset = KeyPointsDataset(validation_keys, test_aug, train=False)\n",
    "\n",
    "print(f\"Total batches in training set: {len(train_dataset)}\")\n",
    "print(f\"Total batches in validation set: {len(validation_dataset)}\")\n",
    "\n",
    "sample_images, sample_keypoints = next(iter(train_dataset))\n",
    "assert sample_keypoints.max() == 1.0\n",
    "assert sample_keypoints.min() == 0.0\n",
    "\n",
    "sample_keypoints = sample_keypoints[:2].reshape(-1, NUM_KEYPOINTS, 2) * IMG_SIZE\n",
    "visualize_keypoints(sample_images[:2], sample_keypoints)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Model building\n",
    "The [Stanford dogs dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/) (on which\n",
    "the StanfordExtra dataset is based) was built using the [ImageNet-1k dataset](http://image-net.org/).\n",
    "So, it is likely that the models pretrained on the ImageNet-1k dataset would be useful\n",
    "for this task. We will use a MobileNetV2 pre-trained on this dataset as a backbone to\n",
    "extract meaningful features from the images and then pass those to a custom regression\n",
    "head for predicting coordinates.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    # Load the pre-trained weights of MobileNetV2 and freeze the weights\n",
    "    backbone = keras.applications.MobileNetV2(\n",
    "        weights=\"imagenet\", include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
    "    )\n",
    "    backbone.trainable = False\n",
    "\n",
    "    inputs = layers.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    x = keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
    "    x = backbone(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.SeparableConv2D(\n",
    "        NUM_KEYPOINTS*2, kernel_size=5, strides=1, activation=\"relu\"\n",
    "    )(x)\n",
    "    outputs = layers.SeparableConv2D(\n",
    "        NUM_KEYPOINTS*2, kernel_size=3, strides=1, activation=\"sigmoid\"\n",
    "    )(x)\n",
    "\n",
    "    return keras.Model(inputs, outputs, name=\"keypoint_detector\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Our custom network is fully-convolutional which makes it more parameter-friendly than the\n",
    "same version of the network having fully-connected dense layers.\n",
    "\"\"\"\n",
    "\n",
    "get_model().summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notice the output shape of the network: `(None, 1, 1, NUM_KEYPOINTS*2)`. This is why we have reshaped\n",
    "the coordinates as: `batch_keypoints[i, :] = np.array(kp_temp).reshape(1, 1, NUM_KEYPOINTS * 2)`.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "## Model compilation and training\n",
    "For this example, we will train the network only for five epochs.\n",
    "\"\"\"\n",
    "\n",
    "model = get_model()\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(1e-3), metrics=[\"accuracy\"])\n",
    "model.fit(train_dataset, validation_data=validation_dataset, epochs=EPOCHS, batch_size=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Make predictions and visualize them\n",
    "\"\"\"\n",
    "sample_val_images, sample_val_keypoints = next(iter(validation_dataset))\n",
    "sample_val_images = sample_val_images[:4]\n",
    "sample_val_keypoints = sample_val_keypoints[:4].reshape(-1, NUM_KEYPOINTS, 2) * IMG_SIZE\n",
    "predictions = model.predict(sample_val_images).reshape(-1, NUM_KEYPOINTS, 2) * IMG_SIZE\n",
    "\n",
    "# Ground-truth\n",
    "visualize_keypoints(sample_val_images, sample_val_keypoints)\n",
    "\n",
    "# Predictions\n",
    "visualize_keypoints(sample_val_images, predictions)\n",
    "\n",
    "\"\"\"\n",
    "Predictions will likely improve with more training.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "## Going further\n",
    "* Try using other augmentation transforms from `imgaug` to investigate how that changes\n",
    "the results.\n",
    "* Here, we transferred the features from the pre-trained network linearly that is we did\n",
    "not [fine-tune](https://keras.io/guides/transfer_learning/) it. You are encouraged to fine-tune it on this task and see if that\n",
    "improves the performance. You can also try different architectures and see how they\n",
    "affect the final performance.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_val_images, sample_val_keypoints = next(iter(train_dataset))\n",
    "sample_val_images = sample_val_images[:4]\n",
    "sample_val_keypoints = sample_val_keypoints[:4].reshape(-1, NUM_KEYPOINTS, 2) * IMG_SIZE\n",
    "raw_predicitions = model.predict(sample_val_images)\n",
    "predictions = raw_predicitions.reshape(-1, NUM_KEYPOINTS, 2) * IMG_SIZE\n",
    "\n",
    "# Ground-truth\n",
    "visualize_keypoints(sample_val_images, sample_val_keypoints)\n",
    "\n",
    "# Predictions\n",
    "visualize_keypoints(sample_val_images, predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}