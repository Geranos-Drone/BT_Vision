{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Title: Keypoint Detection with Transfer Learning for Pole detection\n",
    "Author: [Nicolas Gorlo, Tim Reinhart]\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Based on:\n",
    "Title: Keypoint Detection with Transfer Learning\n",
    "Author: [Sayak Paul](https://twitter.com/RisingSayak)\n",
    "Date created: 2021/05/02\n",
    "Last modified: 2021/05/02\n",
    "Description: Training a keypoint detector with data augmentation and transfer learning.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "## Imports\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from imgaug.augmentables.kps import KeypointsOnImage\n",
    "from imgaug.augmentables.kps import Keypoint\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import getpass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Define hyperparameters\n",
    "\"\"\"\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 600\n",
    "NUM_KEYPOINTS = 7 # 7 keypoints each having x and y coordinates"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Load data\n",
    "\"\"\"\n",
    "\n",
    "USERNAME = getpass.getuser()\n",
    "IMG_DIR = \"/home/\" + USERNAME + \"/BT_Vision/convert_to_coco/vicon_dataset_coco/images/train\"\n",
    "IMG_DIR_VAL = \"/home/\" + USERNAME + \"/BT_Vision/convert_to_coco/vicon_dataset_coco/images/val\"\n",
    "JSON = \"/home/\" + USERNAME + \"/BT_Vision/convert_to_coco/vicon_dataset_coco/annotations/pole_keypoints_7_train.json\"\n",
    "JSON_VAL = \"/home/\" + USERNAME + \"/BT_Vision/convert_to_coco/vicon_dataset_coco/annotations/pole_keypoints_7_val.json\"\n",
    "KEYPOINT_DEF = (\n",
    "    \"keypoint_definitions.csv\"\n",
    ")\n",
    "\n",
    "# Load the ground-truth annotations.\n",
    "with open(JSON) as infile:\n",
    "    coco_data = json.load(infile)\n",
    "\n",
    "with open(JSON_VAL) as infile:\n",
    "    coco_data_val = json.load(infile)\n",
    "\n",
    "# Set up a dictionary, mapping all the ground-truth information\n",
    "# with respect to the path of the image.\n",
    "\n",
    "def coco_to_json_data(coco_data, IMG_DIR):\n",
    "    coco_annotations = coco_data[\"annotations\"]\n",
    "    coco_images = coco_data[\"images\"]\n",
    "    json_data = []\n",
    "    for entry in coco_annotations:\n",
    "        img_bbox = entry[\"bbox\"]\n",
    "        joints = []\n",
    "        for i in range(NUM_KEYPOINTS):\n",
    "            joints.append([entry[\"keypoints\"][3*i],entry[\"keypoints\"][3*i+1],entry[\"keypoints\"][3*i+2]])\n",
    "        for img in coco_images:\n",
    "            if img[\"id\"] == entry[\"image_id\"]:\n",
    "                image_name = img[\"file_name\"]\n",
    "                image_path = os.path.join(IMG_DIR, image_name)\n",
    "                img_width = img[\"width\"]\n",
    "                img_height = img[\"height\"]\n",
    "        json_data_entry = {\n",
    "            \"img_bbox\": img_bbox,\n",
    "            \"img_height\": img_height,\n",
    "            \"img_width\": img_width,\n",
    "            \"img_path\": image_path,\n",
    "            \"joints\": joints,\n",
    "        }\n",
    "        json_data.append(json_data_entry)\n",
    "    json_dict = {i[\"img_path\"]: i for i in json_data}\n",
    "    return json_data, json_dict\n",
    "\n",
    "\n",
    "json_data, json_dict = coco_to_json_data(coco_data, IMG_DIR)\n",
    "json_data_val, json_dict_val = coco_to_json_data(coco_data_val, IMG_DIR_VAL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the metdata definition file and preview it.\n",
    "keypoint_def = pd.read_csv(KEYPOINT_DEF)\n",
    "keypoint_def.head()\n",
    "\n",
    "# Extract the colours and labels.\n",
    "colours = keypoint_def[\"Hex_colour\"].values.tolist()\n",
    "colours = [\"#\" + colour for colour in colours]\n",
    "labels = keypoint_def[\"Name\"].values.tolist()\n",
    "\n",
    "# Utility for reading an image and for getting its annotations.\n",
    "\n",
    "def get_pole(name):\n",
    "    for i in json_dict.keys():\n",
    "        if i == name:\n",
    "            data = json_dict[name]\n",
    "    for i in json_dict_val.keys():\n",
    "        if i == name:\n",
    "            data = json_dict_val[name]\n",
    "    img_data = plt.imread(os.path.join(IMG_DIR, data[\"img_path\"]))\n",
    "    # If the image is RGBA convert it to RGB.\n",
    "    if img_data.shape[-1] == 4:\n",
    "        img_data = img_data.astype(np.uint8)\n",
    "        img_data = Image.fromarray(img_data)\n",
    "        img_data = np.array(img_data.convert(\"RGB\"))\n",
    "    data[\"img_data\"] = img_data\n",
    "\n",
    "    return data\n",
    "\n",
    "# Load the metdata definition file and preview it.\n",
    "keypoint_def = pd.read_csv(KEYPOINT_DEF)\n",
    "keypoint_def.head()\n",
    "\n",
    "# Extract the colours and labels.\n",
    "colours = keypoint_def[\"Hex_colour\"].values.tolist()\n",
    "colours = [\"#\" + colour for colour in colours]\n",
    "labels = keypoint_def[\"Name\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Visualize data\n",
    "\"\"\"\n",
    "\n",
    "# Parts of this code come from here:\n",
    "# https://github.com/benjiebob/StanfordExtra/blob/master/demo.ipynb\n",
    "def visualize_keypoints(images, keypoints):\n",
    "    fig, axes = plt.subplots(nrows=len(images), ncols=2, figsize=(16, 12))\n",
    "    [ax.axis(\"off\") for ax in np.ravel(axes)]\n",
    "\n",
    "    for (ax_orig, ax_all), image, current_keypoint in zip(axes, images, keypoints):\n",
    "        ax_orig.imshow(image)\n",
    "        ax_all.imshow(image)\n",
    "\n",
    "        # If the keypoints were formed by `imgaug` then the coordinates need\n",
    "        # to be iterated differently.\n",
    "        if isinstance(current_keypoint, KeypointsOnImage):\n",
    "            for idx, kp in enumerate(current_keypoint.keypoints):\n",
    "                ax_all.scatter(\n",
    "                    [kp.x], [kp.y], c=colours[idx], marker=\"x\", s=50, linewidths=5\n",
    "                )\n",
    "        else:\n",
    "            current_keypoint = np.array(current_keypoint)\n",
    "            # Since the last entry is the visibility flag, we discard it.\n",
    "            current_keypoint = current_keypoint[:, :2]\n",
    "            for idx, (x, y) in enumerate(current_keypoint):\n",
    "                ax_all.scatter([x], [y], c=colours[idx], marker=\"x\", s=50, linewidths=5)\n",
    "\n",
    "    plt.tight_layout(pad=2.0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Select four samples randomly for visualization.\n",
    "samples = list(json_dict.keys())\n",
    "samples_val = list(json_dict_val.keys())\n",
    "num_samples = 4\n",
    "selected_samples = np.random.choice(samples, num_samples, replace=False)\n",
    "\n",
    "images, keypoints = [], []\n",
    "\n",
    "for sample in selected_samples:\n",
    "    data = get_pole(sample)\n",
    "    image = data[\"img_data\"]\n",
    "    keypoint = data[\"joints\"]\n",
    "\n",
    "    images.append(image)\n",
    "    keypoints.append(keypoint)\n",
    "\n",
    "visualize_keypoints(images, keypoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Prepare data generator\n",
    "\"\"\"\n",
    "\n",
    "class KeyPointsDataset(keras.utils.Sequence):\n",
    "    def __init__(self, image_keys, aug, batch_size=BATCH_SIZE, train=True):\n",
    "        self.image_keys = image_keys\n",
    "        self.aug = aug\n",
    "        self.batch_size = batch_size\n",
    "        self.train = train\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_keys) // self.batch_size\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.image_keys))\n",
    "        if self.train:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        image_keys_temp = [self.image_keys[k] for k in indexes]\n",
    "        (images, keypoints) = self.__data_generation(image_keys_temp)\n",
    "\n",
    "        return (images, keypoints)\n",
    "\n",
    "    def __data_generation(self, image_keys_temp):\n",
    "        batch_images = np.empty((self.batch_size, IMG_SIZE, IMG_SIZE, 3), dtype=\"int\")\n",
    "        batch_keypoints = np.empty(\n",
    "            (self.batch_size, 1, 1, NUM_KEYPOINTS*2), dtype=\"float32\"\n",
    "        )\n",
    "        for i, key in enumerate(image_keys_temp):\n",
    "            data = get_pole(key)\n",
    "            current_keypoint = np.array(data[\"joints\"])[:, :2]\n",
    "            kps = []\n",
    "\n",
    "            # To apply our data augmentation pipeline, we first need to\n",
    "            # form Keypoint objects with the original coordinates.\n",
    "            for j in range(0, len(current_keypoint)):\n",
    "                kps.append(Keypoint(x=current_keypoint[j][0], y=current_keypoint[j][1]))\n",
    "\n",
    "            # We then project the original image and its keypoint coordinates.\n",
    "            current_image = data[\"img_data\"]\n",
    "            kps_obj = KeypointsOnImage(kps, shape=current_image.shape)\n",
    "\n",
    "            # Apply the augmentation pipeline.\n",
    "            (new_image, new_kps_obj) = self.aug(image=current_image, keypoints=kps_obj)\n",
    "            batch_images[\n",
    "                i,\n",
    "            ] = new_image\n",
    "\n",
    "            # Parse the coordinates from the new keypoint object.\n",
    "            kp_temp = []\n",
    "            for keypoint in new_kps_obj:\n",
    "                kp_temp.append(np.nan_to_num(keypoint.x))\n",
    "                kp_temp.append(np.nan_to_num(keypoint.y))\n",
    "\n",
    "            # reshape to fit output shape of the network `(None, 1, 1, NUM_KEYPOINTS*2)`\n",
    "            batch_keypoints[i,] = np.array(\n",
    "                kp_temp\n",
    "            ).reshape(1, 1, NUM_KEYPOINTS * 2)\n",
    "\n",
    "        # Scale the coordinates to [0, 1] range.\n",
    "        batch_keypoints = batch_keypoints / IMG_SIZE\n",
    "\n",
    "        return (batch_images, batch_keypoints)\n",
    "\n",
    "# doc keypoints in imgaug: [this document](https://imgaug.readthedocs.io/en/latest/source/examples_keypoints.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Define augmentation transforms\n",
    "\"\"\"\n",
    "\n",
    "train_aug = iaa.Sequential(\n",
    "    [\n",
    "        iaa.Resize(IMG_SIZE, interpolation=\"linear\"),\n",
    "        #iaa.Fliplr(0.3),\n",
    "        # `Sometimes()` applies a function randomly to the inputs with\n",
    "        # a given probability (0.3, in this case).\n",
    "        iaa.OneOf([\n",
    "            iaa.Sometimes(0.4, iaa.Affine(rotate=(-20,20), scale=(0.5, 0.7))),\n",
    "            iaa.Sometimes(0.4, iaa.Affine(translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}\n",
    "                                          ,scale = 0.6)),\n",
    "            iaa.Sometimes(0.4, iaa.Affine(translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}\n",
    "                                          ,scale=(0.3,0.45),rotate=(-10,10)))\n",
    "        ])\n",
    "        #iaa.LinearContrast((0.75, 1.5)),\n",
    "        #iaa.Multiply((0.5, 1.0), per_channel=0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_aug = iaa.Sequential([iaa.Resize(IMG_SIZE, interpolation=\"linear\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Create training and validation splits\n",
    "\"\"\"\n",
    "\n",
    "np.random.shuffle(samples)\n",
    "np.random.shuffle(samples_val)\n",
    "\n",
    "train_keys = samples\n",
    "validation_keys = samples_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Data generator investigation\n",
    "\"\"\"\n",
    "\n",
    "train_dataset = KeyPointsDataset(train_keys, train_aug)\n",
    "validation_dataset = KeyPointsDataset(validation_keys, test_aug, train=False)\n",
    "\n",
    "print(f\"Total batches in training set: {len(train_dataset)}\")\n",
    "print(f\"Total batches in validation set: {len(validation_dataset)}\")\n",
    "\n",
    "sample_images, sample_keypoints = next(iter(train_dataset))\n",
    "assert sample_keypoints.max() <= 1.0\n",
    "assert sample_keypoints.min() >= 0.0\n",
    "\n",
    "sample_keypoints = sample_keypoints[:2].reshape(-1, NUM_KEYPOINTS, 2) * IMG_SIZE\n",
    "#visualize_keypoints(sample_images[:2], sample_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Model building\n",
    "\"\"\"\n",
    "\n",
    "def get_model():\n",
    "    # Load the pre-trained weights of MobileNetV2 and freeze the weights\n",
    "    backbone = keras.applications.MobileNetV2(\n",
    "        weights=\"imagenet\", include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
    "    )\n",
    "    backbone.trainable = False\n",
    "\n",
    "    inputs = layers.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    x = keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
    "    x = backbone(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.SeparableConv2D(\n",
    "        NUM_KEYPOINTS*2, kernel_size=5, strides=1, activation=\"relu\"\n",
    "    )(x)\n",
    "    outputs = layers.SeparableConv2D(\n",
    "        NUM_KEYPOINTS*2, kernel_size=3, strides=1, activation=\"sigmoid\"\n",
    "    )(x)\n",
    "\n",
    "    return keras.Model(inputs, outputs, name=\"keypoint_detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fully-convolutional makes it more parameter-friendly than the\n",
    "same version of the network having fully-connected dense layers.\n",
    "\"\"\"\n",
    "\n",
    "get_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "## Model compilation and training\n",
    "\"\"\"\n",
    "\n",
    "callback = EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                min_delta=0,\n",
    "                patience=20,\n",
    "                verbose=1,\n",
    "                mode=\"auto\",\n",
    "                baseline=None,\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "\n",
    "model = get_model()\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(1e-4), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(train_dataset, validation_data=validation_dataset, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FINE TUNING:\n",
    "\"\"\"\n",
    "\n",
    "model.trainable = True\n",
    "\n",
    "checkpoint_filepath = '/tmp/checkpoints'\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_accuracy',\n",
    "    mode='auto',\n",
    "    save_best_only=False\n",
    ")\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(1e-5), metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#load checkpoint:\n",
    "\n",
    "model = keras.models.load_model(\"/tmp/checkpoints/\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    model.fit(train_dataset, validation_data=validation_dataset, epochs=20, batch_size=BATCH_SIZE, callbacks=[model_checkpoint_callback])\n",
    "except Exception as e:\n",
    "    print(\"Error: \", e)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Save Model in Network folder\n",
    "model.save(\"network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import network\n",
    "\"\"\"\n",
    "model = keras.models.load_model('network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Make predictions and visualize them\n",
    "\"\"\"\n",
    "sample_val_images, sample_val_keypoints = next(iter(validation_dataset))\n",
    "sample_val_images = sample_val_images[:4]\n",
    "sample_val_keypoints = sample_val_keypoints[:4].reshape(-1, NUM_KEYPOINTS, 2) * IMG_SIZE\n",
    "predictions = model.predict(sample_val_images).reshape(-1, NUM_KEYPOINTS, 2) * IMG_SIZE\n",
    "\n",
    "# Ground-truth\n",
    "visualize_keypoints(sample_val_images, sample_val_keypoints)\n",
    "\n",
    "# Predictions\n",
    "visualize_keypoints(sample_val_images, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## next steps\n",
    "* more \"imgaug\" augmentations\n",
    "* Fine tune features [fine-tune](https://keras.io/guides/transfer_learning/) it.    DONE\n",
    "* adapt model (check coco keypoint detection challenge for inspirations)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PNP:\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "points_2d = predictions[1]\n",
    "\n",
    "points_3d = np.array([\n",
    "                    (0.0, 0.0, 0.0),\n",
    "                    (-0.075, 0.0, -0.2),\n",
    "                    (0.075, 0.0, -0.2),\n",
    "                    (-0.075, 0.0, -0.7),\n",
    "                    (0.075, 0.0, -0.7),\n",
    "                    (-0.075, 0.0, -1.3),\n",
    "                    (-0.075, 0.0, -1.3),\n",
    "                    ])\n",
    "\n",
    "camera_Matrix = np.array([(347.5293999809815 * 224/640, 0.0, 314.7548267525618 * 224/640),\n",
    "                         (0.0, 347.45033648440716 * 224/480, 247.32551331252066 * 224/480),\n",
    "                         (0.0, 0.0, 1.0)])\n",
    "\n",
    "dist_coeffs = np.array([-0.06442475368146962, 0.10266027381230053, -0.16303799346444728, 0.08403964035356283])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "success, rotation_vec, translation_vec = cv2.solvePnP(points_3d, points_2d, camera_Matrix, dist_coeffs)\n",
    "#print(rotation_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nose_end_point2d, jacobian = cv2.projectPoints(np.array([(0.0,0.0,-1.0)]), rotation_vec, translation_vec, camera_Matrix, dist_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image = np.ascontiguousarray(sample_val_images[1], dtype=np.uint8)\n",
    "\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "for p in points_2d:\n",
    "    cv2.circle(image, (int(p[0]), int(p[1])), 3, (0,0,255) )\n",
    "\n",
    "point1 = ( int(points_2d[0][0]), int(points_2d[0][1]) )\n",
    "point2 = ( int(nose_end_point2d[0][0][0]), int(nose_end_point2d[0][0][1]) )\n",
    "\n",
    "cv2.line(image, point1, point2, (255,255,255), 2)\n",
    "cv2.imshow('test', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d37b30281fc82d0d4351911c9d3720bc6567563bbd57a2a36a9a988e3c0b799"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}