{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af644a9b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'path = os.path.join(\\'test_dataset\\',\\'annotations\\',\\'ls_export.json\\')\\n\\nwith open(path) as json_file:\\n    data_ = json.load(json_file)\\n\\n\\nfor it in range(len(data_)):\\n    img_name_ = data_[it][\\'file_upload\\'].split(sep=\\'_\\')[1]\\n    img_id_ = int(img_name_.split(sep=\\'.\\')[0])\\n    data_[it][\\'id\\'] = img_id_\\n    data_[it][\\'annotations\\'][0][\\'id\\'] = img_id_\\n\\ndata = data_\\nim_id = 1002\\n\\nfor i_1 in range(len(data)):\\n    print(data[i_1][\\'file_upload\\'].split(sep = \\'_\\')[-1].split(sep = \".\")[0])\\n    if int(data[i_1][\\'file_upload\\'].split(sep = \\'_\\')[-1].split(sep = \".\")[0]) == im_id:\\n        print(\"1002\")'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Convert json files in labelstudio json format into json file with COCO keypoint format\n",
    "\"\"\"\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from shutil import copyfile\n",
    "import json\n",
    "import argparse\n",
    "import cv2\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "POLE_KEYPOINTS = [\n",
    "    'star',        #1\n",
    "    'rhomb',       #2\n",
    "    'torp',        #3\n",
    "    'tear',        #4\n",
    "    'arrow',       #5\n",
    "    'triangle',    #6\n",
    "    'ninja',       #7\n",
    "    'bolt'         #8\n",
    "]\n",
    "\n",
    "POLE_SKELETON = [\n",
    "    [1,2],[1,4],[1,5],[2,3],[2,6],[3,4],[3,7],[4,8],[5,8],[5,6],[6,7],[7,8]\n",
    "]\n",
    "\n",
    "def cli():\n",
    "    parser = argparse.ArgumentParser(description=__doc__,\n",
    "                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('--dir_data', default='data-poles/train',\n",
    "                        help='dataset directory')\n",
    "    parser.add_argument('--dir_out', default='data-poles',\n",
    "                        help='where to save annotations and files')\n",
    "    \n",
    "    parser.add_argument('--sample', action='store_true',\n",
    "                        help='Whether to only process the first 50 images')\n",
    "    parser.add_argument('--single_sample', action='store_true',\n",
    "                        help='Whether to only process the first image')\n",
    "    \n",
    "    parser.add_argument('--split_images', action='store_true',\n",
    "                        help='Whether to copy images into train val split folder')\n",
    "    parser.add_argument('--histogram', action='store_true',\n",
    "                        help='Whether to show keypoints histogram')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "\n",
    "class LSJsonToCoco:\n",
    "    \n",
    "    \n",
    "    sample = False\n",
    "    single_sample = False\n",
    "    split_images = False\n",
    "    histogram = False\n",
    "    \n",
    "    def __init__(self, dir_dataset, dir_out):\n",
    "        \n",
    "        assert os.path.isdir(dir_dataset), 'dataset directory not found'\n",
    "        self.dir_dataset = dir_dataset\n",
    "        \n",
    "        \n",
    "        self.dir_out_im = os.path.join(dir_out, 'images')\n",
    "        self.dir_out_ann = os.path.join(dir_out, 'annotations')\n",
    "        os.makedirs(self.dir_out_im, exist_ok=True)\n",
    "        os.makedirs(self.dir_out_ann, exist_ok=True)\n",
    "        \n",
    "        self.coco_file = {}\n",
    "        \n",
    "        # Load train val split\n",
    "        path_train = os.path.join(self.dir_dataset, 'split', 'train.txt') #train-list.text: list of train images\n",
    "        path_val = os.path.join(self.dir_dataset, 'split', 'val.txt') #validation-list.text: list of validation images\n",
    "        self.splits = {}\n",
    "        for name, path in zip(('train', 'val'), (path_train, path_val)): #zip creates an iterator over all input iterables\n",
    "            with open(path, \"r\") as ff:\n",
    "                lines = ff.readlines()\n",
    "            self.splits[name] = [os.path.join(self.dir_dataset, 'images', line.strip())\n",
    "                                 for line in lines]\n",
    "            assert self.splits[name], \"specified path is empty\"\n",
    "        \n",
    "        \n",
    "    def process(self):\n",
    "        img_ids = []\n",
    "        annotation_id = 10000\n",
    "        \n",
    "        data = self._load_lsjson(os.path.join(self.dir_dataset,'annotations','ls_export.json'))\n",
    "        \n",
    "        for phase, im_paths in self.splits.items():\n",
    "            \n",
    "            cnt_images = 0\n",
    "            cnt_instances = 0\n",
    "            cnt_kps = [0] * len(POLE_KEYPOINTS)\n",
    "            self.initiate_coco()  # Initiate coco json file at each phase\n",
    "            \n",
    "            if self.sample:\n",
    "                im_paths = im_paths[:50]\n",
    "            if self.split_images:\n",
    "                path_dir = (os.path.join(self.dir_out_im, phase))\n",
    "                os.makedirs(path_dir, exist_ok=True)\n",
    "                assert not os.listdir(path_dir), \"Directory to save images is not empty. \" \\\n",
    "                    \"Remove flag --split_images ?\"\n",
    "            elif self.single_sample:\n",
    "                im_paths = self.splits['train'][:1]\n",
    "                print(f'Single sample for train/val:{im_paths}')\n",
    "            \n",
    "            for im_path in im_paths:\n",
    "                im_size, im_name, im_id = self._process_image(im_path,phase)\n",
    "                cnt_images += 1\n",
    "                \n",
    "                annotation_id += 1\n",
    "                self._process_annotation(data, im_size, im_id, cnt_kps, annotation_id)\n",
    "                cnt_instances += 1\n",
    "            \n",
    "                if self.split_images:\n",
    "                    dst = os.path.join(self.dir_out_im, phase, os.path.split(im_path)[-1])\n",
    "                    copyfile(im_path, dst)\n",
    "\n",
    "                # Count\n",
    "                if (cnt_images % 1000) == 0:\n",
    "                    text = ' and copied to new directory' if self.split_images else ''\n",
    "                    print(f'Parsed {cnt_images} images' + text)\n",
    "                \n",
    "        \n",
    "            self.save_coco_files(phase)\n",
    "            print(f'\\nPhase:{phase}')\n",
    "            print(f'Average number of keypoints labelled: {sum(cnt_kps) / cnt_instances:.1f} / {len(POLE_KEYPOINTS)}')\n",
    "            print(f'COCO files directory:  {self.dir_out_ann}')\n",
    "            print(f'Saved {cnt_instances} instances over {cnt_images} images ')\n",
    "            if self.histogram:\n",
    "                histogram(cnt_kps)    \n",
    "        \n",
    "        \n",
    "    def initiate_coco(self):\n",
    "        \"\"\"\n",
    "        Initiate Coco File for Training and validation phase\n",
    "        \"\"\"\n",
    "        for co_file in [(self.coco_file)]:\n",
    "            co_file[\"info\"] = dict(url=\"https://github.com/openpifpaf/openpifpaf\",\n",
    "                                  date_created=time.strftime(\"%a, %d %b %Y %H:%M:%S +0000\",\n",
    "                                                             time.localtime()),\n",
    "                                  description=(\"Conversion of Label-Studio Pole dataset into MS-COCO format\"))\n",
    "\n",
    "            skel = POLE_SKELETON\n",
    "            kps = POLE_KEYPOINTS\n",
    "            co_file[\"categories\"] = [dict(name='pole',\n",
    "                                         id=420,\n",
    "                                         skeleton=skel,\n",
    "                                         supercategory='poles',\n",
    "                                         keypoints=kps)]\n",
    "            co_file[\"images\"] = []\n",
    "            co_file[\"annotations\"] = []\n",
    "            \n",
    "            \n",
    "    def save_coco_files(self, phase):\n",
    "        for co_file, n_kps in [(self.coco_file, len(POLE_KEYPOINTS))]:\n",
    "            name = 'pole_keypoints_' + str(n_kps) + '_'\n",
    "            if self.sample:\n",
    "                name = name + 'sample_'\n",
    "            elif self.single_sample:\n",
    "                name = name + 'single_sample_'\n",
    "\n",
    "            path_coco = os.path.join(self.dir_out_ann, name + phase + '.json')\n",
    "            with open(path_coco, 'w') as outfile:\n",
    "                json.dump(co_file, outfile)\n",
    "            \n",
    "        \n",
    "    def _process_image(self, im_path, phase):\n",
    "        \"\"\"Update image field in json file\"\"\"\n",
    "        file_name = os.path.basename(im_path)\n",
    "        im_name = os.path.splitext(file_name)[0] # \n",
    "        im_id = int(im_name.split(sep='_')[1])  # Numeric code in the image\n",
    "        im = Image.open(im_path)\n",
    "        width, height = im.size\n",
    "        dict_img = {\n",
    "            'coco_url': \"unknown\",\n",
    "            'file_name': file_name,\n",
    "            'id': im_id,\n",
    "            'license': 1,\n",
    "            'date_captured': \"unknown\",\n",
    "            'width': width,\n",
    "            'height': height}\n",
    "        self.coco_file[\"images\"].append(dict_img)\n",
    "\n",
    "        return (width, height), im_name, im_id\n",
    "        \n",
    "    \n",
    "    def _process_annotation(self, data, im_size, im_id, cnt_kps, annotation_id):\n",
    "        \"\"\"Process single instance of annotations\"\"\"\n",
    "        \n",
    "        for i_1 in range(len(data)):\n",
    "            if int(data[i_1]['file_upload'].split(sep = '_')[-1].split(sep = \".\")[0]) == im_id:\n",
    "                \n",
    "                keypoints_coco = []\n",
    "                bbox_inst = []\n",
    "                area_inst = []\n",
    "                annotation_inst = data[i_1].get('annotations')\n",
    "                result_inst = annotation_inst[0].get('result')\n",
    "                i_2 = 0\n",
    "                x_i = []\n",
    "                y_i = []\n",
    "                im_size = [result_inst[0]['original_width'],result_inst[0]['original_height']]\n",
    "                for label_index, label_inst in enumerate(POLE_KEYPOINTS):\n",
    "                    if (i_2 < len(result_inst)) and (result_inst[i_2]['value']['keypointlabels'][0] == label_inst):\n",
    "                        x_ii = result_inst[i_2]['value']['x']*im_size[0]/100\n",
    "                        y_ii = result_inst[i_2]['value']['y']*im_size[1]/100\n",
    "                        keypoints_coco.append(x_ii)\n",
    "                        keypoints_coco.append(y_ii)\n",
    "                        x_i.append(x_ii)\n",
    "                        y_i.append(y_ii)\n",
    "                        keypoints_coco.append(2)\n",
    "                        cnt_kps[label_index] += 1\n",
    "\n",
    "                        i_2 += 1\n",
    "                    else:\n",
    "                        keypoints_coco.append(0)\n",
    "                        keypoints_coco.append(0)\n",
    "                        keypoints_coco.append(0)\n",
    "\n",
    "                box_tight = [np.min(x_i), np.min(y_i), np.max(x_i), np.max(y_i)]\n",
    "                w, h = box_tight[2] - box_tight[0], box_tight[3] - box_tight[1]\n",
    "                x_o = max(box_tight[0] - 0.1 * w, 0)\n",
    "                y_o = max(box_tight[1] - 0.1 * h, 0)\n",
    "                x_i = min(box_tight[0] + 1.1 * w, im_size[0])\n",
    "                y_i = min(box_tight[1] + 1.1 * h, im_size[1])\n",
    "                bbox_inst = [int(x_o), int(y_o), int(x_i - x_o), int(y_i - y_o)]  # (x, y, w, h)\n",
    "\n",
    "                \n",
    "                dict_ann ={\n",
    "                    \"segmentation\":[],\n",
    "                    \"num_keypoints\": len(POLE_KEYPOINTS),\n",
    "                    \"iscrowd\": 0,\n",
    "                    \"keypoints\": keypoints_coco,\n",
    "                    \"image_id\": im_id,\n",
    "                    \"category_id\": 420,\n",
    "                    \"id\": annotation_id,\n",
    "                    \"area\": bbox_inst[2]*bbox_inst[3],\n",
    "                    \"bbox\": bbox_inst}\n",
    "                self.coco_file[\"annotations\"].append(dict_ann)\n",
    "        \n",
    "        return cnt_kps\n",
    "    \n",
    "    def _load_lsjson(self, path):\n",
    "        with open(path) as json_file:\n",
    "            data_ = json.load(json_file)\n",
    "\n",
    "               \n",
    "        for it in range(len(data_)):\n",
    "            img_name_ = data_[it]['file_upload'].split(sep='_')[1]\n",
    "            img_id_ = int(img_name_.split(sep='.')[0])\n",
    "            data_[it]['id'] = img_id_\n",
    "            data_[it]['annotations'][0]['id'] = img_id_\n",
    "\n",
    "            \n",
    "        return data_\n",
    "\n",
    "def histogram(cnt_kps):\n",
    "    bins = np.arange(len(cnt_kps))\n",
    "    data = np.array(cnt_kps)\n",
    "    plt.figure()\n",
    "    plt.title(\"Distribution of the keypoints\")\n",
    "    plt.bar(bins, data)\n",
    "    plt.xticks(np.arange(len(cnt_kps), step=5))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def main():\n",
    "    \n",
    "    args = cli()\n",
    "    LSJsonToCoco.sample = args.sample\n",
    "    LSJsonToCoco.single_sample = args.single_sample\n",
    "    LSJsonToCoco.split_images = args.split_images\n",
    "    LSJsonToCoco.histogram = args.histogram\n",
    "    \n",
    "    \n",
    "    lsjson_coco = LSJsonToCoco(args.dir_data, args.dir_out)\n",
    "    lsjson_coco.process()\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\"\"\"path = os.path.join('test_dataset','annotations','ls_export.json')\n",
    "\n",
    "with open(path) as json_file:\n",
    "    data_ = json.load(json_file)\n",
    "\n",
    "\n",
    "for it in range(len(data_)):\n",
    "    img_name_ = data_[it]['file_upload'].split(sep='_')[1]\n",
    "    img_id_ = int(img_name_.split(sep='.')[0])\n",
    "    data_[it]['id'] = img_id_\n",
    "    data_[it]['annotations'][0]['id'] = img_id_\n",
    "\n",
    "data = data_\n",
    "im_id = 1002\n",
    "\n",
    "for i_1 in range(len(data)):\n",
    "    print(data[i_1]['file_upload'].split(sep = '_')[-1].split(sep = \".\")[0])\n",
    "    if int(data[i_1]['file_upload'].split(sep = '_')[-1].split(sep = \".\")[0]) == im_id:\n",
    "        print(\"1002\")\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcf92f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
